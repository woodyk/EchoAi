#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# File: deep_research.py
# Author: Wadih Khairallah
# Description: 
# Created: 2025-03-25 14:21:35
# Modified: 2025-03-27 16:21:42

import asyncio
import time
import json
import re
from rich.console import Console
from rich.rule import Rule
from interactor import Interactor

console = Console()

# Global cache for LLM responses to avoid duplicate calls
cache = {}

async def async_interact(llm, prompt, cache_key=None, **kwargs):
    """
    Asynchronous wrapper for llm.interact() using asyncio.to_thread.
    Returns a cached result if available.
    """
    key = cache_key if cache_key is not None else prompt
    if key in cache:
        console.print(f"[blue]Using cached result for prompt:[/blue] {key}")
        return cache[key]
    try:
        result = await asyncio.to_thread(llm.interact, prompt, **kwargs)
        cache[key] = result
        return result
    except Exception as e:
        console.print(f"[red]Error during LLM interaction for prompt:[/red] {prompt}\nError: {e}")
        return ""

def parse_sub_tasks(response: str):
    """
    Parse the LLM's response into a list of sub-questions.
    
    Strategy:
      1. Try to parse the response as JSON.
      2. If that fails, look for text enclosed within <task>...</task> tags.
      3. If no tags are found, extract lines that end with a question mark.
    """
    # Attempt to parse JSON first
    try:
        tasks = json.loads(response)
        if isinstance(tasks, list):
            return [str(task).strip() for task in tasks if str(task).strip()]
    except json.JSONDecodeError:
        pass

    # Try to extract tasks using <task> tags
    tag_matches = re.findall(r"<task>(.*?)<\/task>", response, re.IGNORECASE | re.DOTALL)
    if tag_matches:
        return [match.strip() for match in tag_matches if match.strip()]

    # Fallback: extract lines that end with a question mark.
    lines = response.splitlines()
    question_lines = [line.strip("- *").strip() for line in lines if line.strip().endswith("?")]
    return question_lines

def assemble_report(sections):
    """
    Combine report sections into a final report with header and footer.
    """
    header = "# Deep Research Report\n\n"
    body = "\n\n".join(sections)
    footer = "\n\n---\nReport generated by Deep Research Agent Prototype."
    return header + body + footer

async def process_sub_task(llm, task):
    """
    Process an individual sub-task: retrieval, analysis, synthesis.
    Measures performance time for each step.
    """
    console.print(f"[green]Processing sub-task:[/green] {task}")
    
    # Retrieval step
    retrieval_prompt = f"Provide detailed information on: {task}"
    start_time = time.perf_counter()
    retrieval_response = await async_interact(llm, retrieval_prompt, cache_key=f"retrieval:{task}", quiet=True)
    retrieval_time = time.perf_counter() - start_time
    console.print(f"[cyan]Retrieval completed in {retrieval_time:.2f} seconds.[/cyan]")
    
    # Analysis step
    analysis_prompt = (
        f"Analyze the following information and extract the key insights for the task '{task}':\n\n{retrieval_response}"
    )
    start_time = time.perf_counter()
    analysis_response = await async_interact(llm, analysis_prompt, cache_key=f"analysis:{task}", quiet=True)
    analysis_time = time.perf_counter() - start_time
    console.print(f"[cyan]Analysis completed in {analysis_time:.2f} seconds.[/cyan]")
    
    # Synthesis step
    synthesis_prompt = (
        f"Compose a well-structured report section for the task '{task}' based on these insights:\n\n"
        f"{analysis_response}\n\nInclude appropriate citations if possible."
    )
    start_time = time.perf_counter()
    synthesis_response = await async_interact(llm, synthesis_prompt, cache_key=f"synthesis:{task}", quiet=True)
    synthesis_time = time.perf_counter() - start_time
    console.print(f"[cyan]Synthesis completed in {synthesis_time:.2f} seconds.[/cyan]")
    
    return synthesis_response

async def deep_research(query):
    # Instantiate our LLM interactor. (Change model as needed.)
    llm = Interactor(model="ollama:mistral-nemo")
    
    console.print("[bold]Starting Deep Research...[/bold]")
    
    # 1. Planning: Break down the query into sub-questions.
    planning_prompt = f"Decompose the following research query into clear, manageable sub-questions: {query}"
    start_time = time.perf_counter()
    planning_response = await async_interact(llm, planning_prompt, cache_key=f"planning:{query}", quiet=True)
    planning_time = time.perf_counter() - start_time
    console.print(f"[cyan]Planning completed in {planning_time:.2f} seconds.[/cyan]")
    console.print(Rule())

    sub_tasks = parse_sub_tasks(planning_response)
    if not sub_tasks:
        console.print("[red]No valid sub-tasks were generated. Exiting.[/red]")
        return ""
    
    # Process all sub-tasks concurrently.
    tasks = [process_sub_task(llm, task) for task in sub_tasks]
    report_sections = await asyncio.gather(*tasks)
    console.print(Rule())
    
    # 3. Compile the final report.
    final_report = assemble_report(report_sections)
    return final_report

if __name__ == "__main__":
    user_query = "What are the latest advances in AI neural network research?"
    final_report = asyncio.run(deep_research(user_query))
    console.print("\n[bold magenta]Final Report:[/bold magenta]\n")
    console.print(final_report)

