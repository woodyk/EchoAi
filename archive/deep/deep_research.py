#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# File: deep_research.py
# Author: Wadih Khairallah
# Description: 
# Created: 2025-03-25 14:20:18

from interactor import Interactor
from rich.console import Console

console = Console()

def deep_research(query):
    # Instantiate our LLM interactor (using your provided script)
    #llm = Interactor(model="openai:gpt-4o-mini")
    llm = Interactor(model="ollama:llama3.2")

    # 1. Planning: Ask the LLM to break down the query into sub-questions.
    planning_prompt = (
        f"Decompose the following research query into clear, manageable sub-questions: {query}"
    )
    planning_response = llm.interact(planning_prompt)
    console.print("\n----------------------------------------------------------\n")
    sub_tasks = parse_sub_tasks(planning_response)
    
    report_sections = []

    # 2. For each sub-task, retrieve data and analyze
    for task in sub_tasks:
        # a. Formulate a search query for the sub-task
        search_query = f"Provide detailed information on: {task}"
        
        # Use the LLM to simulate a retrieval step (or integrate an external retrieval function here)
        retrieval_response = llm.interact(search_query)
        console.print("\n----------------------------------------------------------\n")
        
        # b. Analysis: Ask the LLM to analyze the retrieved information
        analysis_prompt = (
            f"Analyze the following information and extract the key insights for the task '{task}':\n\n{retrieval_response}"
        )
        analysis = llm.interact(analysis_prompt)
        console.print("\n----------------------------------------------------------\n")
        
        # c. Synthesis: Ask the LLM to format this analysis as a report section
        synthesis_prompt = (
            f"Compose a well-structured report section for the task '{task}' based on these insights:\n\n{analysis}\n\nInclude appropriate citations if possible."
        )
        section = llm.interact(synthesis_prompt)
        console.print("\n----------------------------------------------------------\n")
        report_sections.append(section)
    
    # 3. Compile the final report
    final_report = assemble_report(report_sections)
    return final_report

def parse_sub_tasks(response: str):
    """
    Parse the LLM's response into a list of sub-questions.
    This is a placeholder for actual parsing logic.
    """
    # Example: simply split by newline or use regex to extract bullet points.
    return [line.strip("- ") for line in response.splitlines() if line.strip()]

def assemble_report(sections):
    """
    Combine report sections into a final report with a header and footer.
    """
    header = "# Deep Research Report\n\n"
    body = "\n\n".join(sections)
    footer = "\n\n---\nReport generated by Deep Research Agent Prototype."
    return header + body + footer

# Example usage:
if __name__ == "__main__":
    user_query = "What are the latest advances in renewable energy research?"
    report = deep_research(user_query)
    console.print(report)
